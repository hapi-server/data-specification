HAPI_spec_issues.txt

Date: 2017-04-11
Add to spec?: never
Issue: Include URL request parameters in the response?
Resolution: This should not be done for security reasons, because it is a way to inject client text into the server response. It is actually not allowed on government sites, as it can facilitate cross-site scripting type attacks.


Date: 2017-04-11
Add to the spec?: needs doing
Issue:
Should "units" be required?
Resolution:
Yes. We will change the "units" attribute to be required, with no default value. The server must specify "dimensionless" if the quantity does not have any units. You can specify null if there really aren't any units. If the type of the parameter is "isotime", then the server must return "UTC" for the units.

Date: 2017-04-11
Add to the spec?: never
Issue:
Should the data header that gets returned include the exact time range of the actual data that it is returning.
Resolution:
No. This might be difficult or impossible to implement for a streaming service. The first record and the last record will have this information, so it is not included in the header.


Date: 2017-04-11
Add to the spec?: needs doing
Issue:
The spec indicates that the data returned must fall exactly within the requested time range - no extra data outside this time range should be returned.
Resolution:
This behavior is clarified to mean the the start time is inclusive, and the stop time is exclusive. Some servers expand the time range sligtly, but the HAPI spec requires a precise selection. This slicing of data in time is deemed to be of the same difficulty as slicing the data by parameters, whcih HAPI servers are already required to do. It also allows data requests for adjoining time ranges to concatenated together without fear of duplication.


Date: 2017-04-11
Add to the spec?: not yet
Issue:
Are timestamps given for the start, middle, or end of the measurment interval?
Resolution:
For now, the spec will be neutral about the location of the time stamp.  The HAPI server just delivers the time stamps as they are in the data with no modifications.
As a suggestion for the future, this could be an optional attribute in the info header for the primary time column -- something to indicate the flavor of time stamp.
Maybe even an enum: (BEGIN, END, MIDDLE, OTHER, UNKNOWN)
{ "parameters": [ {"name": "Epoch", "type":"isotime", "timeStampLocation":"middle"} ] }



Issues in progress:

Date: 2017-04-11
Add to the spec?: issue in progress
Issue:
What is the best way to indicate an error status to clients?
Discussion:
We decided to use the JSON header mechanism to communicate error messages back to clients.
The HTTP error codes are still to be used:
200 - OK
400 - bad request
500 - internal server error
But the status of the response is indicated in a "status" attribute in the JSON header.
This has multiple implcations. First, and most obviously, if the client did not request the header, then it will not see the status message. So a request for data with no header is to be viewed as a higher-risk operation with more low level error checking needed (i.e., checking of the HTTP error codes, which is more difficult, depending on which tool is being used to access the HAPI URLs.
Second, there needs to be an official way to indicate an OK status.
Third, there needs to be a place for a human readable status message.
The discussion centered around if or how to use status codes.
Should these be the same as the HTTP codes? Some were uncomfortable jumping into the name space of the existing HTTP codes.
Also, we could use the status codes to indicate the common situation of a valid request that simply returns no data.
We could include the httpCode and the hapiCode both in the response. We experimented with a new "status" attribute that might look like the following for various situations:
We need to decide if the "data" entity is returned even when the status is bad.
For the "no data" status, it seems to make sense to return an empty data element, 
     "data" : []
confirming to clients that everything worked, but there were just no records for the given time interval.

Here are some examples of what the status attribute might look like:

everything works:
 "status" : {"httpCode": 200, "hapiCode": 1200, "message": "OK" }

ok, but no data:
  "status" : {"httpCode": 200, "hapiCode": 1210, "message": "OK - no data in time range" }

request-based error:
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "unparsable start time" }
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "unparsable stop time" }
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "start time after end time" }
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "start time after end time" }
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "time outside valid range for dataset" }
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "unknown dataset id" }
  "status" : {"httpCode": 400, "hapiCode": 1400, "message": "unknown dataset parameter" }

internal error / bug:
   "status" : {"httpCode": 500, "hapiCode": 1500, "message": "internal server error" } 

We had arbitrarily used HAPI codes of 600, 700, 800 during the telecon, but maybe we just add 1000 to the HTTP codes to make HAPI codes:
codes in the 1200 range - everything is OK
codes in the 1400 range - bad request
codes in the 1500 range - internal server problem
(This is all still very much in discussion!!!)

Servers can sometimes return HTTP 500 codes, but not always, if the problem is catastrophic.

Even with a 400 response code, you can still return an entity body (i.e., the JSON header message)

If the status is not 600
recommendations of messages for specific error conditions:



Date: 2017-04-11
Add to the spec?: issue in progress
Issue:
What about createing a HAPIU-branded way to get to metadata?
Discussion:
This was introduced at the end, and will need lots of clarification.
The main idea is to provide some way to link the resourceURL, an optional
attribute in a header, to a resolver that can get to the metadata content using
the resourceID.
There are two different opinions on this:
1. Todd is thinking about having HAPI data access be under a new name space, so instead of 
http://server.com/hapi/catalog
http://server.com/hapi/info
http://server.com/hapi/data
http://server.com/hapi/capabilities
It would all move to an "access" level:
http://server.com/hapi/access/catalog
http://server.com/hapi/access/info
http://server.com/hapi/access/data
http://server.com/hapi/access/capabilities
to distinguish it from a future "registry" level - another HAPI-branded mechanism that would be a generic way to get to any kind of metadata registry.

2. Jon is opposed to adding anything like this to HAPI, since it is an orthoganal concern. A resolver is a completely separate system, and should not be mixed into a data access server. Although this new mechanism was refered to as a registry, the registry that we talked about in our plan for this year's activities was simply a way to list available HAPI servers, and had nothing to do with accessing SPASE or other metadata.


Other topics still to resolve:
--------------------------------
a. multi-dimensional arrays - what about having bins for some and not others?
b. Jeremy's plan for representing time-varying bins
c. our plan for this year: the 8 projects to work on
d. another feature request: for each dataset, there should be some way of indicating that the server will limit the request, either to a max number of records or to a maximum time range



